# ü§ü Sign Language Recognition System - Real-time Translation to Text and Speech

Real-time sign language recognition and translation system using Deep Learning and Computer Vision.

## üìã Project Description

This project implements a complete sign language recognition system that captures gestures via webcam, processes them using MediaPipe to extract keypoints from body, hands, and face, and classifies them using an LSTM neural network to translate them into text and speech.

## üß† Arquitectura de la Red Neuronal

### Tipo de Red
**LSTM (Long Short-Term Memory)** - Red Neuronal Recurrente especializada en secuencias temporales

### Arquitectura del Modelo

```
Input: (15 frames, 1662 keypoints)
    ‚Üì
LSTM Layer 1: 64 unidades + Dropout (0.5) + L2 Regularization (0.01)
    ‚Üì
LSTM Layer 2: 128 unidades + Dropout (0.5) + L2 Regularization (0.001)
    ‚Üì
Dense Layer 1: 64 neuronas + ReLU + L2 Regularization (0.001)
    ‚Üì
Dense Layer 2: 64 neuronas + ReLU + L2 Regularization (0.001)
    ‚Üì
Output Layer: N clases + Softmax
```

**Par√°metros clave:**
- **Frames por secuencia:** 15 frames normalizados
- **Keypoints totales:** 1,662 puntos por frame
  - Pose: 33 landmarks √ó 4 coordenadas (x, y, z, visibility) = 132
  - Rostro: 468 landmarks √ó 3 coordenadas (x, y, z) = 1,404
  - Mano izquierda: 21 landmarks √ó 3 coordenadas = 63
  - Mano derecha: 21 landmarks √ó 3 coordenadas = 63
- **Optimizador:** Adam
- **Funci√≥n de p√©rdida:** Categorical Crossentropy
- **M√©trica:** Accuracy

### Regularizaci√≥n
- **Dropout:** 50% para prevenir overfitting
- **L2 Regularization:** Aplicada en todas las capas LSTM y Dense
- **Early Stopping:** Paciencia de 10 √©pocas monitoreando accuracy

## üî¨ Algoritmos y T√©cnicas Utilizadas

### 1. Extracci√≥n de Caracter√≠sticas
- **MediaPipe Holistic:** Detecci√≥n de 1,662 puntos clave en tiempo real
  - Pose estimation (33 puntos)
  - Face mesh (468 puntos)
  - Hand tracking bilateral (42 puntos total)

### 2. Preprocesamiento de Datos
- **Normalizaci√≥n temporal:** Interpolaci√≥n/submuestreo a 15 frames fijos
  - Interpolaci√≥n lineal para secuencias cortas
  - Submuestreo uniforme para secuencias largas
- **Padding:** Pre-padding con ceros para secuencias variables
- **Normalizaci√≥n de datos:** Conversi√≥n a float16 para eficiencia

### 3. Detecci√≥n de Inicio/Fin de Se√±a
- **Algoritmo de ventana deslizante:**
  - Margen de frames: 1 frame
  - Delay de confirmaci√≥n: 3 frames
  - M√≠nimo de frames: 5 frames
  - Detecci√≥n basada en presencia de manos

### 4. Clasificaci√≥n
- **Umbral de confianza:** 70-80% para aceptar predicci√≥n
- **Softmax:** Probabilidades normalizadas para cada clase
- **Argmax:** Selecci√≥n de clase con mayor probabilidad

### 5. Post-procesamiento
- **Text-to-Speech:** Google TTS (gTTS) para s√≠ntesis de voz en espa√±ol
- **Pygame:** Reproducci√≥n de audio generado

## üìÅ Estructura del Proyecto

```
modelo_lstm_lsp/
‚îú‚îÄ‚îÄ üìÑ Archivos Principales
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Interfaz GUI con PyQt5
‚îÇ   ‚îú‚îÄ‚îÄ capture_samples.py         # Captura de muestras de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ normalize_samples.py       # Normalizaci√≥n de frames a 15 frames
‚îÇ   ‚îú‚îÄ‚îÄ create_keypoints.py        # Extracci√≥n de keypoints con MediaPipe
‚îÇ   ‚îú‚îÄ‚îÄ training_model.py          # Entrenamiento del modelo LSTM
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_model.py          # Evaluaci√≥n y pruebas del modelo
‚îÇ   ‚îî‚îÄ‚îÄ confusion_matrix.py        # Generaci√≥n de matriz de confusi√≥n
‚îÇ
‚îú‚îÄ‚îÄ üõ†Ô∏è Archivos de Soporte
‚îÇ   ‚îú‚îÄ‚îÄ model.py                   # Definici√≥n de arquitectura LSTM
‚îÇ   ‚îú‚îÄ‚îÄ helpers.py                 # Funciones auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ constants.py               # Constantes y configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ text_to_speech.py          # Conversi√≥n texto a voz
‚îÇ   ‚îú‚îÄ‚îÄ server.py                  # API Flask para procesamiento de videos
‚îÇ   ‚îî‚îÄ‚îÄ process_video.py           # Procesamiento de videos externos
‚îÇ
‚îú‚îÄ‚îÄ üé® Interfaz
‚îÇ   ‚îú‚îÄ‚îÄ mainwindow.ui              # Dise√±o de interfaz Qt (v1)
‚îÇ   ‚îî‚îÄ‚îÄ mainwindow_2.ui            # Dise√±o de interfaz Qt (v2)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Directorios de Datos
‚îÇ   ‚îú‚îÄ‚îÄ frame_actions/             # Frames capturados por palabra
‚îÇ   ‚îú‚îÄ‚îÄ data/                      # Datos procesados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keypoints/             # Archivos .h5 con keypoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.json              # Metadatos
‚îÇ   ‚îî‚îÄ‚îÄ models/                    # Modelos entrenados
‚îÇ       ‚îú‚îÄ‚îÄ actions_15.keras       # Modelo LSTM entrenado
‚îÇ       ‚îî‚îÄ‚îÄ words.json             # IDs de palabras reconocidas
‚îÇ
‚îî‚îÄ‚îÄ üìã Configuraci√≥n
    ‚îú‚îÄ‚îÄ requirements.txt           # Dependencias del proyecto
    ‚îî‚îÄ‚îÄ README.md                  # Este archivo
```

## üöÄ Instalaci√≥n

### Requisitos Previos
- Python 3.8 - 3.10 (recomendado 3.8 por compatibilidad con TensorFlow 2.10)
- Webcam funcional
- Windows/Linux/MacOS

### Instalaci√≥n de Dependencias

```bash
# Clonar el repositorio
git clone https://github.com/itzsebas121/Sing-to-text-python
cd Sing-to-text-python

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Crear estructura de directorios necesaria
mkdir -p data/keypoints frame_actions  # Linux/Mac
# En Windows: mkdir data\keypoints, mkdir frame_actions
```

### Configuraci√≥n Inicial Despu√©s de Clonar

> **‚ö†Ô∏è IMPORTANTE:** El repositorio NO incluye datos de entrenamiento ni modelos pre-entrenados debido a su gran tama√±o. Despu√©s de clonar, tienes **dos opciones**:

#### Opci√≥n 1: Entrenar tu Propio Modelo (Recomendado para Aprender) üéì

Esta opci√≥n te permite entender todo el proceso desde cero:

1. **Capturar tus propias muestras** para cada palabra que quieras reconocer
2. **Normalizar** las muestras capturadas
3. **Generar keypoints** de las muestras
4. **Entrenar** el modelo LSTM
5. **Evaluar** el modelo entrenado

```bash
# Sigue el flujo completo descrito en la secci√≥n "Gu√≠a de Uso"
python capture_samples.py      # Paso 1
python normalize_samples.py    # Paso 2
python create_keypoints.py     # Paso 3
python training_model.py       # Paso 4
python evaluate_model.py       # Paso 5
```

**Tiempo estimado:** 2-4 horas (dependiendo de cu√°ntas palabras captures)

#### Opci√≥n 2: Descargar Modelo Pre-entrenado (Inicio R√°pido) ‚ö°

Si solo quieres probar el sistema sin entrenar:

1. Descarga el modelo pre-entrenado y datos desde [enlace-a-releases] *(pr√≥ximamente)*
2. Extrae los archivos en las carpetas correspondientes:
   - `models/actions_15.keras` - Modelo entrenado
   - `models/words.json` - Lista de palabras (ya incluido en el repo)
   - `data/keypoints/*.h5` - Keypoints de entrenamiento (opcional)
3. Ejecuta directamente:

```bash
python evaluate_model.py  # Prueba en tiempo real
# o
python main.py           # Interfaz GUI
```

**Nota:** El modelo pre-entrenado reconoce las palabras listadas en la secci√≥n "Palabras Reconocidas Actualmente".


### Dependencias Principales
```
tensorflow==2.10.1          # Framework de Deep Learning
keras==2.10.0               # API de alto nivel para TensorFlow
mediapipe==0.10.11          # Detecci√≥n de pose y manos
opencv-contrib-python==4.9.0.80  # Procesamiento de im√°genes
numpy==1.26.4               # Operaciones num√©ricas
pandas==2.2.2               # Manejo de datos
PyQt5==5.15.9               # Interfaz gr√°fica
gTTS==2.5.1                 # Text-to-Speech
pygame==2.5.2               # Reproducci√≥n de audio
Flask==3.0.2                # API REST (opcional)
tables==3.9.2               # Manejo de archivos HDF5
protobuf==3.20.3            # Serializaci√≥n de datos
```

## üìñ Gu√≠a de Uso

### Flujo de Trabajo Completo

#### 1Ô∏è‚É£ Captura de Muestras
```bash
python capture_samples.py
```
- Modifica la variable `word_name` en el script para la palabra a capturar
- Realiza la se√±a frente a la c√°mara m√∫ltiples veces (recomendado: 50-100 muestras)
- Las muestras se guardan en `frame_actions/<palabra>/sample_<timestamp>/`
- Presiona 'q' para salir

**Consejos:**
- Var√≠a la velocidad de ejecuci√≥n de la se√±a
- Cambia ligeramente la posici√≥n y √°ngulo
- Usa diferentes iluminaciones
- Captura con ambas manos si aplica

#### 2Ô∏è‚É£ Normalizaci√≥n de Frames
```bash
python normalize_samples.py
```
- Normaliza todas las muestras a exactamente 15 frames
- Usa interpolaci√≥n para secuencias cortas
- Usa submuestreo para secuencias largas
- Sobrescribe los frames originales

#### 3Ô∏è‚É£ Generaci√≥n de Keypoints
```bash
python create_keypoints.py
```
- Extrae los 1,662 keypoints de cada frame usando MediaPipe
- Genera archivos `.h5` en `data/keypoints/<palabra>.h5`
- Procesa todas las palabras en `frame_actions/` por defecto
- Muestra progreso en tiempo real

#### 4Ô∏è‚É£ Entrenamiento del Modelo
```bash
python training_model.py
```
- Entrena la red LSTM con todas las palabras disponibles
- Par√°metros por defecto: 500 √©pocas m√°ximo, early stopping con paciencia 10
- Divisi√≥n: 95% entrenamiento, 5% validaci√≥n
- Guarda el modelo en `models/actions_15.keras`
- Muestra resumen del modelo y m√©tricas

**Personalizaci√≥n:**
```python
# En training_model.py
training_model(MODEL_PATH, epochs=1000)  # Cambiar n√∫mero de √©pocas
```

#### 5Ô∏è‚É£ Evaluaci√≥n del Modelo
```bash
python evaluate_model.py
```
- Prueba el modelo en tiempo real con la c√°mara
- Muestra predicciones con porcentaje de confianza
- Reproduce audio de las palabras reconocidas
- Umbral de confianza: 80% por defecto
- Presiona 'q' para salir

**Par√°metros ajustables:**
```python
evaluate_model(src=None, threshold=0.8, margin_frame=1, delay_frames=3)
# src: None para c√°mara, o ruta de video
# threshold: umbral de confianza (0.0-1.0)
```

#### 6Ô∏è‚É£ Interfaz Gr√°fica (GUI)
```bash
python main.py
```
- Interfaz PyQt5 con visualizaci√≥n en tiempo real
- Muestra keypoints sobre el video
- Traduce se√±as a texto y voz autom√°ticamente
- Acumula palabras reconocidas en la interfaz

#### 7Ô∏è‚É£ Matriz de Confusi√≥n (Opcional)
```bash
python confusion_matrix.py
```
- Genera matriz de confusi√≥n para evaluar el modelo
- Visualiza errores de clasificaci√≥n entre clases
- √ötil para identificar se√±as que se confunden

#### 8Ô∏è‚É£ API REST (Opcional)
```bash
python server.py
```
- Inicia servidor Flask en `http://0.0.0.0:5000`
- Endpoint: `POST /upload_video` - Procesa videos y retorna traducci√≥n
- √ötil para integraci√≥n con aplicaciones m√≥viles/web

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Modificar Par√°metros del Modelo
Edita `constants.py`:
```python
MIN_LENGTH_FRAMES = 5      # M√≠nimo de frames para detectar se√±a
LENGTH_KEYPOINTS = 1662    # Total de keypoints (NO MODIFICAR)
MODEL_FRAMES = 15          # Frames por secuencia (requiere reentrenamiento)
```

### Agregar Nuevas Palabras
1. Edita `constants.py` y agrega la palabra a `words_text`:
```python
words_text = {
    "hola": "HOLA",
    "adios": "ADIOS",
    "nueva_palabra": "NUEVA PALABRA",  # Agregar aqu√≠
}
```
2. Captura muestras con `capture_samples.py`
3. Normaliza con `normalize_samples.py`
4. Genera keypoints con `create_keypoints.py`
5. Reentrena el modelo con `training_model.py`

### Ajustar Arquitectura del Modelo
Edita `model.py`:
```python
def get_model(max_length_frames, output_length: int):
    model = Sequential()
```

### Error: Incompatibilidad de TensorFlow
- Aseg√∫rate de usar Python 3.8-3.10
- TensorFlow 2.10 requiere protobuf 3.20.x

### C√°mara no detectada
- Cambia el √≠ndice de c√°mara en los scripts:
```python
video = cv2.VideoCapture(0)  # Prueba con 0, 1, 2, etc.
```

### Modelo no reconoce se√±as
- Verifica que el umbral no sea muy alto (reduce de 0.8 a 0.6)
- Aseg√∫rate de tener suficientes muestras de entrenamiento (>50 por palabra)
- Revisa que la iluminaci√≥n sea adecuada
- Confirma que las manos sean visibles en el frame

### Error al cargar modelo .keras
- Verifica que el archivo exista en `models/actions_15.keras`
- Reentrena el modelo si es necesario

## üìà Mejoras Futuras

- [ ] Implementar atenci√≥n (Attention Mechanism) en LSTM
- [ ] Agregar m√°s palabras al vocabulario
- [ ] Implementar traducci√≥n de frases completas
- [ ] Optimizar para dispositivos m√≥viles (TensorFlow Lite)
- [ ] Mejorar detecci√≥n con transformers
- [ ] Agregar soporte para otras lenguas de se√±as
- [ ] Implementar data augmentation para mejorar generalizaci√≥n

## üé• Video Tutorial

Explicaci√≥n detallada del c√≥digo: [https://youtu.be/3EK0TxfoAMk](https://youtu.be/3EK0TxfoAMk)

*Nota: Pr√≥ximamente video con las mejoras implementadas*

## üìÑ Licencia

Este proyecto es de c√≥digo abierto y est√° disponible para fines educativos y de investigaci√≥n.

## üë®‚Äçüíª Contribuciones

Las contribuciones son bienvenidas. Por favor:
1. Haz fork del proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìß Contacto

Para preguntas, sugerencias o colaboraciones, por favor abre un issue en el repositorio.

---

**Desarrollado con ‚ù§Ô∏è para la comunidad sorda**

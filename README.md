# ğŸ¤Ÿ Traductor de Lengua de SeÃ±as Peruana (LSP) a Texto y Voz

Sistema de reconocimiento y traducciÃ³n en tiempo real de Lengua de SeÃ±as Peruana (LSP) utilizando Deep Learning y Computer Vision.

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa un sistema completo de reconocimiento de seÃ±as peruanas que captura gestos mediante cÃ¡mara web, los procesa utilizando MediaPipe para extraer puntos clave (keypoints) del cuerpo, manos y rostro, y los clasifica mediante una red neuronal LSTM para traducirlos a texto y voz en espaÃ±ol.

## ğŸ§  Arquitectura de la Red Neuronal

### Tipo de Red
**LSTM (Long Short-Term Memory)** - Red Neuronal Recurrente especializada en secuencias temporales

### Arquitectura del Modelo

```
Input: (15 frames, 1662 keypoints)
    â†“
LSTM Layer 1: 64 unidades + Dropout (0.5) + L2 Regularization (0.01)
    â†“
LSTM Layer 2: 128 unidades + Dropout (0.5) + L2 Regularization (0.001)
    â†“
Dense Layer 1: 64 neuronas + ReLU + L2 Regularization (0.001)
    â†“
Dense Layer 2: 64 neuronas + ReLU + L2 Regularization (0.001)
    â†“
Output Layer: N clases + Softmax
```

**ParÃ¡metros clave:**
- **Frames por secuencia:** 15 frames normalizados
- **Keypoints totales:** 1,662 puntos por frame
  - Pose: 33 landmarks Ã— 4 coordenadas (x, y, z, visibility) = 132
  - Rostro: 468 landmarks Ã— 3 coordenadas (x, y, z) = 1,404
  - Mano izquierda: 21 landmarks Ã— 3 coordenadas = 63
  - Mano derecha: 21 landmarks Ã— 3 coordenadas = 63
- **Optimizador:** Adam
- **FunciÃ³n de pÃ©rdida:** Categorical Crossentropy
- **MÃ©trica:** Accuracy

### RegularizaciÃ³n
- **Dropout:** 50% para prevenir overfitting
- **L2 Regularization:** Aplicada en todas las capas LSTM y Dense
- **Early Stopping:** Paciencia de 10 Ã©pocas monitoreando accuracy

## ğŸ”¬ Algoritmos y TÃ©cnicas Utilizadas

### 1. ExtracciÃ³n de CaracterÃ­sticas
- **MediaPipe Holistic:** DetecciÃ³n de 1,662 puntos clave en tiempo real
  - Pose estimation (33 puntos)
  - Face mesh (468 puntos)
  - Hand tracking bilateral (42 puntos total)

### 2. Preprocesamiento de Datos
- **NormalizaciÃ³n temporal:** InterpolaciÃ³n/submuestreo a 15 frames fijos
  - InterpolaciÃ³n lineal para secuencias cortas
  - Submuestreo uniforme para secuencias largas
- **Padding:** Pre-padding con ceros para secuencias variables
- **NormalizaciÃ³n de datos:** ConversiÃ³n a float16 para eficiencia

### 3. DetecciÃ³n de Inicio/Fin de SeÃ±a
- **Algoritmo de ventana deslizante:**
  - Margen de frames: 1 frame
  - Delay de confirmaciÃ³n: 3 frames
  - MÃ­nimo de frames: 5 frames
  - DetecciÃ³n basada en presencia de manos

### 4. ClasificaciÃ³n
- **Umbral de confianza:** 70-80% para aceptar predicciÃ³n
- **Softmax:** Probabilidades normalizadas para cada clase
- **Argmax:** SelecciÃ³n de clase con mayor probabilidad

### 5. Post-procesamiento
- **Text-to-Speech:** Google TTS (gTTS) para sÃ­ntesis de voz en espaÃ±ol
- **Pygame:** ReproducciÃ³n de audio generado

## ğŸ“ Estructura del Proyecto

```
modelo_lstm_lsp/
â”œâ”€â”€ ğŸ“„ Archivos Principales
â”‚   â”œâ”€â”€ main.py                    # Interfaz GUI con PyQt5
â”‚   â”œâ”€â”€ capture_samples.py         # Captura de muestras de entrenamiento
â”‚   â”œâ”€â”€ normalize_samples.py       # NormalizaciÃ³n de frames a 15 frames
â”‚   â”œâ”€â”€ create_keypoints.py        # ExtracciÃ³n de keypoints con MediaPipe
â”‚   â”œâ”€â”€ training_model.py          # Entrenamiento del modelo LSTM
â”‚   â”œâ”€â”€ evaluate_model.py          # EvaluaciÃ³n y pruebas del modelo
â”‚   â””â”€â”€ confusion_matrix.py        # GeneraciÃ³n de matriz de confusiÃ³n
â”‚
â”œâ”€â”€ ğŸ› ï¸ Archivos de Soporte
â”‚   â”œâ”€â”€ model.py                   # DefiniciÃ³n de arquitectura LSTM
â”‚   â”œâ”€â”€ helpers.py                 # Funciones auxiliares
â”‚   â”œâ”€â”€ constants.py               # Constantes y configuraciÃ³n
â”‚   â”œâ”€â”€ text_to_speech.py          # ConversiÃ³n texto a voz
â”‚   â”œâ”€â”€ server.py                  # API Flask para procesamiento de videos
â”‚   â””â”€â”€ process_video.py           # Procesamiento de videos externos
â”‚
â”œâ”€â”€ ğŸ¨ Interfaz
â”‚   â”œâ”€â”€ mainwindow.ui              # DiseÃ±o de interfaz Qt (v1)
â”‚   â””â”€â”€ mainwindow_2.ui            # DiseÃ±o de interfaz Qt (v2)
â”‚
â”œâ”€â”€ ğŸ“‚ Directorios de Datos
â”‚   â”œâ”€â”€ frame_actions/             # Frames capturados por palabra
â”‚   â”œâ”€â”€ data/                      # Datos procesados
â”‚   â”‚   â”œâ”€â”€ keypoints/             # Archivos .h5 con keypoints
â”‚   â”‚   â””â”€â”€ data.json              # Metadatos
â”‚   â””â”€â”€ models/                    # Modelos entrenados
â”‚       â”œâ”€â”€ actions_15.keras       # Modelo LSTM entrenado
â”‚       â””â”€â”€ words.json             # IDs de palabras reconocidas
â”‚
â””â”€â”€ ğŸ“‹ ConfiguraciÃ³n
    â”œâ”€â”€ requirements.txt           # Dependencias del proyecto
    â””â”€â”€ README.md                  # Este archivo
```

## ğŸš€ InstalaciÃ³n

### Requisitos Previos
- Python 3.8 - 3.10 (recomendado 3.8 por compatibilidad con TensorFlow 2.10)
- Webcam funcional
- Windows/Linux/MacOS

### InstalaciÃ³n de Dependencias

```bash
# Clonar el repositorio
git clone <url-del-repositorio>
cd modelo_lstm_lsp

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Crear estructura de directorios necesaria
mkdir -p data/keypoints frame_actions  # Linux/Mac
# En Windows: mkdir data\keypoints, mkdir frame_actions
```

### ConfiguraciÃ³n Inicial DespuÃ©s de Clonar

> **âš ï¸ IMPORTANTE:** El repositorio NO incluye datos de entrenamiento ni modelos pre-entrenados debido a su gran tamaÃ±o. DespuÃ©s de clonar, tienes **dos opciones**:

#### OpciÃ³n 1: Entrenar tu Propio Modelo (Recomendado para Aprender) ğŸ“

Esta opciÃ³n te permite entender todo el proceso desde cero:

1. **Capturar tus propias muestras** para cada palabra que quieras reconocer
2. **Normalizar** las muestras capturadas
3. **Generar keypoints** de las muestras
4. **Entrenar** el modelo LSTM
5. **Evaluar** el modelo entrenado

```bash
# Sigue el flujo completo descrito en la secciÃ³n "GuÃ­a de Uso"
python capture_samples.py      # Paso 1
python normalize_samples.py    # Paso 2
python create_keypoints.py     # Paso 3
python training_model.py       # Paso 4
python evaluate_model.py       # Paso 5
```

**Tiempo estimado:** 2-4 horas (dependiendo de cuÃ¡ntas palabras captures)

#### OpciÃ³n 2: Descargar Modelo Pre-entrenado (Inicio RÃ¡pido) âš¡

Si solo quieres probar el sistema sin entrenar:

1. Descarga el modelo pre-entrenado y datos desde [enlace-a-releases] *(prÃ³ximamente)*
2. Extrae los archivos en las carpetas correspondientes:
   - `models/actions_15.keras` - Modelo entrenado
   - `models/words.json` - Lista de palabras (ya incluido en el repo)
   - `data/keypoints/*.h5` - Keypoints de entrenamiento (opcional)
3. Ejecuta directamente:

```bash
python evaluate_model.py  # Prueba en tiempo real
# o
python main.py           # Interfaz GUI
```

**Nota:** El modelo pre-entrenado reconoce las palabras listadas en la secciÃ³n "Palabras Reconocidas Actualmente".


### Dependencias Principales
```
tensorflow==2.10.1          # Framework de Deep Learning
keras==2.10.0               # API de alto nivel para TensorFlow
mediapipe==0.10.11          # DetecciÃ³n de pose y manos
opencv-contrib-python==4.9.0.80  # Procesamiento de imÃ¡genes
numpy==1.26.4               # Operaciones numÃ©ricas
pandas==2.2.2               # Manejo de datos
PyQt5==5.15.9               # Interfaz grÃ¡fica
gTTS==2.5.1                 # Text-to-Speech
pygame==2.5.2               # ReproducciÃ³n de audio
Flask==3.0.2                # API REST (opcional)
tables==3.9.2               # Manejo de archivos HDF5
protobuf==3.20.3            # SerializaciÃ³n de datos
```

## ğŸ“– GuÃ­a de Uso

### Flujo de Trabajo Completo

#### 1ï¸âƒ£ Captura de Muestras
```bash
python capture_samples.py
```
- Modifica la variable `word_name` en el script para la palabra a capturar
- Realiza la seÃ±a frente a la cÃ¡mara mÃºltiples veces (recomendado: 50-100 muestras)
- Las muestras se guardan en `frame_actions/<palabra>/sample_<timestamp>/`
- Presiona 'q' para salir

**Consejos:**
- VarÃ­a la velocidad de ejecuciÃ³n de la seÃ±a
- Cambia ligeramente la posiciÃ³n y Ã¡ngulo
- Usa diferentes iluminaciones
- Captura con ambas manos si aplica

#### 2ï¸âƒ£ NormalizaciÃ³n de Frames
```bash
python normalize_samples.py
```
- Normaliza todas las muestras a exactamente 15 frames
- Usa interpolaciÃ³n para secuencias cortas
- Usa submuestreo para secuencias largas
- Sobrescribe los frames originales

#### 3ï¸âƒ£ GeneraciÃ³n de Keypoints
```bash
python create_keypoints.py
```
- Extrae los 1,662 keypoints de cada frame usando MediaPipe
- Genera archivos `.h5` en `data/keypoints/<palabra>.h5`
- Procesa todas las palabras en `frame_actions/` por defecto
- Muestra progreso en tiempo real

#### 4ï¸âƒ£ Entrenamiento del Modelo
```bash
python training_model.py
```
- Entrena la red LSTM con todas las palabras disponibles
- ParÃ¡metros por defecto: 500 Ã©pocas mÃ¡ximo, early stopping con paciencia 10
- DivisiÃ³n: 95% entrenamiento, 5% validaciÃ³n
- Guarda el modelo en `models/actions_15.keras`
- Muestra resumen del modelo y mÃ©tricas

**PersonalizaciÃ³n:**
```python
# En training_model.py
training_model(MODEL_PATH, epochs=1000)  # Cambiar nÃºmero de Ã©pocas
```

#### 5ï¸âƒ£ EvaluaciÃ³n del Modelo
```bash
python evaluate_model.py
```
- Prueba el modelo en tiempo real con la cÃ¡mara
- Muestra predicciones con porcentaje de confianza
- Reproduce audio de las palabras reconocidas
- Umbral de confianza: 80% por defecto
- Presiona 'q' para salir

**ParÃ¡metros ajustables:**
```python
evaluate_model(src=None, threshold=0.8, margin_frame=1, delay_frames=3)
# src: None para cÃ¡mara, o ruta de video
# threshold: umbral de confianza (0.0-1.0)
```

#### 6ï¸âƒ£ Interfaz GrÃ¡fica (GUI)
```bash
python main.py
```
- Interfaz PyQt5 con visualizaciÃ³n en tiempo real
- Muestra keypoints sobre el video
- Traduce seÃ±as a texto y voz automÃ¡ticamente
- Acumula palabras reconocidas en la interfaz

#### 7ï¸âƒ£ Matriz de ConfusiÃ³n (Opcional)
```bash
python confusion_matrix.py
```
- Genera matriz de confusiÃ³n para evaluar el modelo
- Visualiza errores de clasificaciÃ³n entre clases
- Ãštil para identificar seÃ±as que se confunden

#### 8ï¸âƒ£ API REST (Opcional)
```bash
python server.py
```
- Inicia servidor Flask en `http://0.0.0.0:5000`
- Endpoint: `POST /upload_video` - Procesa videos y retorna traducciÃ³n
- Ãštil para integraciÃ³n con aplicaciones mÃ³viles/web

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Modificar ParÃ¡metros del Modelo
Edita `constants.py`:
```python
MIN_LENGTH_FRAMES = 5      # MÃ­nimo de frames para detectar seÃ±a
LENGTH_KEYPOINTS = 1662    # Total de keypoints (NO MODIFICAR)
MODEL_FRAMES = 15          # Frames por secuencia (requiere reentrenamiento)
```

### Agregar Nuevas Palabras
1. Edita `constants.py` y agrega la palabra a `words_text`:
```python
words_text = {
    "hola": "HOLA",
    "adios": "ADIOS",
    "nueva_palabra": "NUEVA PALABRA",  # Agregar aquÃ­
}
```
2. Captura muestras con `capture_samples.py`
3. Normaliza con `normalize_samples.py`
4. Genera keypoints con `create_keypoints.py`
5. Reentrena el modelo con `training_model.py`

### Ajustar Arquitectura del Modelo
Edita `model.py`:
```python
def get_model(max_length_frames, output_length: int):
    model = Sequential()
    model.add(LSTM(128, return_sequences=True, ...))  # Cambiar unidades
    model.add(Dropout(0.3))  # Ajustar dropout
    # ... modificar capas segÃºn necesidad
```

## ğŸ“Š Palabras Reconocidas Actualmente

El modelo estÃ¡ entrenado para reconocer las siguientes seÃ±as LSP:
- **hola** - Saludo bÃ¡sico
- **adios** - Despedida
- **aplausos** - Gesto de aplaudir
- **gusto_conocerte** - Frase de cortesÃ­a
- **nombre** - Pregunta por el nombre
- **hasta_luego** - Despedida temporal
- **zanahoria** - Objeto/alimento
- **seÃ±or** - TÃ­tulo de cortesÃ­a

*Nota: Puedes expandir el vocabulario siguiendo los pasos de la secciÃ³n "Agregar Nuevas Palabras"*

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "No module named 'mediapipe'"
```bash
pip install mediapipe==0.10.11
```

### Error: Incompatibilidad de TensorFlow
- AsegÃºrate de usar Python 3.8-3.10
- TensorFlow 2.10 requiere protobuf 3.20.x

### CÃ¡mara no detectada
- Cambia el Ã­ndice de cÃ¡mara en los scripts:
```python
video = cv2.VideoCapture(0)  # Prueba con 0, 1, 2, etc.
```

### Modelo no reconoce seÃ±as
- Verifica que el umbral no sea muy alto (reduce de 0.8 a 0.6)
- AsegÃºrate de tener suficientes muestras de entrenamiento (>50 por palabra)
- Revisa que la iluminaciÃ³n sea adecuada
- Confirma que las manos sean visibles en el frame

### Error al cargar modelo .keras
- Verifica que el archivo exista en `models/actions_15.keras`
- Reentrena el modelo si es necesario

## ğŸ“ˆ Mejoras Futuras

- [ ] Implementar atenciÃ³n (Attention Mechanism) en LSTM
- [ ] Agregar mÃ¡s palabras al vocabulario
- [ ] Implementar traducciÃ³n de frases completas
- [ ] Optimizar para dispositivos mÃ³viles (TensorFlow Lite)
- [ ] Mejorar detecciÃ³n con transformers
- [ ] Agregar soporte para otras lenguas de seÃ±as
- [ ] Implementar data augmentation para mejorar generalizaciÃ³n

## ğŸ¥ Video Tutorial

ExplicaciÃ³n detallada del cÃ³digo: [https://youtu.be/3EK0TxfoAMk](https://youtu.be/3EK0TxfoAMk)

*Nota: PrÃ³ximamente video con las mejoras implementadas*

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para fines educativos y de investigaciÃ³n.

## ğŸ‘¨â€ğŸ’» Contribuciones

Las contribuciones son bienvenidas. Por favor:
1. Haz fork del proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“§ Contacto

Para preguntas, sugerencias o colaboraciones, por favor abre un issue en el repositorio.

---

**Desarrollado con â¤ï¸ para la comunidad sorda**
